import os
import streamlit as st
import openai
import streamlit as st
from dotenv import load_dotenv  
import src.aoai_helpers as aoai

load_dotenv()  

openai.api_type = "azure"  
openai.api_key = os.environ['APIM_KEY']  
openai.api_base = os.environ['APIM_ENDPOINT']  
openai.api_version = os.environ['AOAI_API_VERSION']

st.title("Interact with a ðŸ¤–")

streaming_options = ["Stream", "Do NOT Stream"]  
selected = st.sidebar.radio("Choose a Streaming or Non-Streaming experience:", streaming_options, key="streamingkey",
                            help=''' No Streaming means results display when the API call is complete.
                            Streaming means results display as they are generated by the API.
                            Streaming is only available for ChatCompletions and Completions. ''')

# Create a dictionary containing the available models for each completion type 
# ###UPDATE 07/12/2023 first round will only accomodate the Chat models until gpt-35-turbo-instruct is released
available_models = {  
    "Chat": ["gpt-4", "gpt-4-32k", "gpt-35-turbo", "gpt-35-turbo-16k"],  
    #"Completion": ["text-davinci-003"],  
    #"Embedding": ["text-embedding-ada-002"]  
}

# These next few lines are unnecessary until other completion types are added
# callEndpoints = ["Chat", "Completion"]#, "Embedding"]  
# completion_type = st.sidebar.radio(  
#     "Choose whether you wish to engage in a Chat (ChatCompletions) or Completions interaction:",# for text or Embeddings for a demonstration of embeddings outputs:",  
#     callEndpoints, key="completionkey", help='''Chat Experience will utilize the GPT-35-Turbo and GPT-4 models to engage in a chat-like conversation. Completion will allow
#     for a single prompt and then response and only works with older model types. Embeddings will provide a response demonstrating what an embedding translation looks like.''')
# Update the model_options depending on the selected completion type  
# model_options = available_models[completion_type]

# ###UPDATE model_options hard-coded to Chat for now 07/12/2023
model_options = available_models["Chat"]

# model_options
model = st.sidebar.selectbox("Choose a model:", model_options, key="modelkey", help='''Choose the model with which you wish to interact.''')

# Then, when a model is selected, load the parameters for that model
params = aoai.model_params.get(model, {})

# Read in the appropriate model specific parameters
temperature = st.sidebar.slider("Set a temperature:", min_value=params['temp_min'], max_value=params['temp_max'], step=params["temp_step"], help=params['temp_help'], key="tempkey")
max_tokens = st.sidebar.slider("Set max_tokens:", min_value=params['tokens_min'], max_value=params['tokens_max'], step=params["tokens_step"], help=params['tokens_help'], key="tokenskey")
top_p = st.sidebar.slider("Set top_p:", min_value=params['top_p_min'], max_value=params['top_p_max'], step=params["top_p_step"], help=params['top_p_help'], key="top_pkey")
frequency_penalty = st.sidebar.slider("Set frequency_penalty:", min_value=params['frequency_penalty_min'], max_value=params['frequency_penalty_max'], step=params["frequency_penalty_step"], help=params['frequency_penalty_help'], key="frequency_penaltykey") 
presence_penalty = st.sidebar.slider("Set presence_penalty:", min_value=params['presence_penalty_min'], max_value=params['presence_penalty_max'], step=params["presence_penalty_step"], help=params['presence_penalty_help'], key="presence_penaltykey")
if selected == "Stream":
    stream = True
else:
    stream = False


st.subheader("ðŸ’¬ Window")

user_input = st.text_input("You: ", placeholder="Ask me anything ...", value="Tell me a short joke", key="input")  

if st.button("Submit", type="primary"):  
    st.markdown("----")  

    res_box = st.empty()  
    exp = st.expander("See more info")  

    messages = []  
    messages.append({"role": "user", "content": user_input})
 
    if stream == True:
        streaming_content = []
        for response in aoai.generate_chat_completion(engine=model,
                                                        messages=messages,
                                                        temperature=temperature,
                                                        max_tokens=max_tokens,
                                                        top_p=top_p,
                                                        frequency_penalty=frequency_penalty,
                                                        presence_penalty=presence_penalty,
                                                        stop=None,
                                                        stream=stream):
            if 'content' in response['choices'][0]['delta']:  
                        streaming_content.append(response['choices'][0]['delta']['content'])  
                        result = "".join(streaming_content).strip()  
                        result = result.replace("\n", "")  
                        res_box.markdown(f'*{result}*') 
    else:
         completion_response = aoai.generate_chat_completion(engine=model,
                                                        messages=messages,
                                                        temperature=temperature,
                                                        max_tokens=max_tokens,
                                                        top_p=top_p,
                                                        frequency_penalty=frequency_penalty,
                                                        presence_penalty=presence_penalty,
                                                        stop=None,
                                                        stream=stream)

st.markdown("----")   
