import os
import streamlit as st
import openai
import streamlit as st
from dotenv import load_dotenv  
import src.aoai_helpers as aoai

load_dotenv()  

openai.api_type = "azure"  
openai.api_key = os.environ['APIM_KEY']  
openai.api_base = os.environ['APIM_ENDPOINT']  
openai.api_version = os.environ['AOAI_API_VERSION']

st.title("Interact with a ðŸ¤–")

streaming_options = ["Stream", "Do NOT Stream"]  
selected = st.sidebar.radio("Choose a Streaming or Non-Streaming experience:", streaming_options, key="streamingkey",
                            help=''' No Streaming means results display when the API call is complete.
                            Streaming means results display as they are generated by the API.
                            Streaming is only available for ChatCompletions and Completions. ''')

# Create a dictionary containing the available models for each completion type 
# ###UPDATE 07/12/2023 first round will only accomodate the Chat models until gpt-35-turbo-instruct is released
available_models = {  
    "Chat": ["gpt-4", "gpt-4-32k", "gpt-35-turbo", "gpt-35-turbo-16k"],  
    #"Completion": ["text-davinci-003"],  
    #"Embedding": ["text-embedding-ada-002"]  
}

# These next few lines are unnecessary until other completion types are added
# callEndpoints = ["Chat", "Completion"]#, "Embedding"]  
# completion_type = st.sidebar.radio(  
#     "Choose whether you wish to engage in a Chat (ChatCompletions) or Completions interaction:",# for text or Embeddings for a demonstration of embeddings outputs:",  
#     callEndpoints, key="completionkey", help='''Chat Experience will utilize the GPT-35-Turbo and GPT-4 models to engage in a chat-like conversation. Completion will allow
#     for a single prompt and then response and only works with older model types. Embeddings will provide a response demonstrating what an embedding translation looks like.''')
# Update the model_options depending on the selected completion type  
# model_options = available_models[completion_type]

# ###UPDATE model_options hard-coded to Chat for now 07/12/2023
model_options = available_models["Chat"]

# model_options
model = st.sidebar.selectbox("Choose a model:", model_options, key="modelkey", help='''Choose the model with which you wish to interact.''')

# Then, when a model is selected, load the parameters for that model
params = aoai.model_params.get(model, {})

temperature = st.sidebar.slider("Set a temperature:", min_value=params['temp_min'], max_value=params['temp_max'], step=params["temp_step"], help=params['temp_help'], key="tempkey")
max_tokens = st.sidebar.slider("Set max_tokens:", min_value=params['tokens_min'], max_value=params['tokens_max'], step=params["tokens_step"], help=params['tokens_help'], key="tokenskey")
top_p = st.sidebar.slider("Set top_p:", min_value=params['top_p_min'], max_value=params['top_p_max'], step=params["top_p_step"], help=params['top_p_help'], key="top_pkey")
frequency_penalty = st.sidebar.slider("Set frequency_penalty:", min_value=params['frequency_penalty_min'], max_value=params['frequency_penalty_max'], step=params["frequency_penalty_step"], help=params['frequency_penalty_help'], key="frequency_penaltykey") 
presence_penalty = st.sidebar.slider("Set presence_penalty:", min_value=params['presence_penalty_min'], max_value=params['presence_penalty_max'], step=params["presence_penalty_step"], help=params['presence_penalty_help'], key="presence_penaltykey")
if selected == "Stream":
    stream = True
else:
    stream = False

# engine, messages, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, stop, stream
st.subheader("ðŸ’¬ Window")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# if prompt := st.chat_input():

#     st.session_state.messages.append({"role": "user", "content": prompt})
#     st.chat_message("user").write(prompt)
#     # response = openai.ChatCompletion.create(engine="gpt-4-32k", messages=st.session_state.messages)
#     response = aoai.generate_chat_completion(engine=model,
#                                              messages=st.session_state.messages,
#                                              temperature=temperature,
#                                              max_tokens=max_tokens,
#                                              top_p=top_p,
#                                              frequency_penalty=frequency_penalty,
#                                              presence_penalty=presence_penalty,
#                                              stop=None,
#                                              stream=stream)
#     msg = response.choices[0].message
#     st.session_state.messages.append(msg)
#     st.chat_message("assistant").write(msg.content)

if prompt := st.chat_input():  
  
    st.session_state.messages.append({"role": "user", "content": prompt})  
    st.chat_message("user").write(prompt)  
  
    response_generator = aoai.generate_chat_completion(engine=model,  
                                             messages=st.session_state.messages,  
                                             temperature=temperature,  
                                             max_tokens=max_tokens,  
                                             top_p=top_p,  
                                             frequency_penalty=frequency_penalty,  
                                             presence_penalty=presence_penalty,  
                                             stop=None,  
                                             stream=stream)  
  
    for response in response_generator:  
        msg = response.choices[0].message  
        st.session_state.messages.append(msg)  
        st.chat_message("assistant").write(msg.content)  

query=st.text_input("input your query",value="Tell me a joke")
ask_button=st.button("ask") 

st.markdown("### streaming box")
# here is the key, setup a empty container first
chat_box=st.empty() 
stream_handler = StreamHandler(chat_box)
chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[stream_handler])

st.markdown("### together box")  

if query and ask_button: 
    response = chat([HumanMessage(content=query)])    
    llm_response = response.content  
    st.markdown(llm_response)